## Seq2seq chatbot with attention mechanism

Основной акцент в этом коде делается на *механизме внимания*, который зарекомендовал себя в задачах связанных с длинными последовательностями. Здесь можно провести аналогию с работой мозга: чем больший по размеру стих мы пытаемся выучить, тем больше нам надо прилагать сил и внимания.

Разработан класс внимания, который включает в себя механизм внимания **Богданова** и 3 варианта механизма внимания **Luong** (*точечное внимание*, *общее внимание* и *объединенное внимание*). Выбор используемого механизма внимания происходит при инициализации класса.

Используемый [датасет](https://storage.yandexcloud.net/academy.ai/LLM/dialogs.txt) - текстовый документ с диалогами.

Подробное описание кода и комментарии к нему описаны внутри **Google Colab** проекта.

> Код нужно запускать с помощью **TPU v2-8** в **Google Colab**!